# 主数据源，默认的
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
spring.datasource.driverClassName=com.mysql.jdbc.Driver
spring.datasource.url=jdbc:mysql://localhost:3306/mybase
spring.datasource.username=root
spring.datasource.password=123456

# 下面为连接池的补充设置，应用到上面所有数据源中
# 初始化大小，最小，最大
spring.datasource.initialSize=5
spring.datasource.minIdle=5
spring.datasource.maxActive=20
# 配置获取连接等待超时的时间
spring.datasource.maxWait=60000
# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
spring.datasource.timeBetweenEvictionRunsMillis=60000
# 配置一个连接在池中最小生存的时间，单位是毫秒
spring.datasource.minEvictableIdleTimeMillis=300000
spring.datasource.validationQuery=SELECT 1 FROMDUAL
spring.datasource.testWhileIdle=true
spring.datasource.testOnBorrow=false
spring.datasource.testOnReturn=false
# 打开PSCache，并且指定每个连接上PSCache的大小
spring.datasource.poolPreparedStatements=true
spring.datasource.maxPoolPreparedStatementPerConnectionSize=20
# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙
spring.datasource.filters=stat,wall,log4j
# 通过connectProperties属性来打开mergeSql功能；慢SQL记录
spring.datasource.connectionProperties=druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
# 合并多个DruidDataSource的监控数据
#spring.datasource.useGlobalDataSourceStat=true

#rabbitmq配置
spring.rabbitmq.host=192.168.8.148
spring.rabbitmq.port=15005
spring.rabbitmq.username=yss
spring.rabbitmq.password=yssp@55

#使用单个redis=redis，redis集群=redisCluster
srping.redis.type=redis

#单个redis
spring.redis.host=localhost
spring.redis.port=6379
spring.redis.password=123456
#spring.redis.host=192.168.2.243
#spring.redis.port=6379
#spring.redis.password=yss
#可用连接实例的最大数目，默认值为8；
#如果赋值为-1，则表示不限制；如果pool已经分配了maxActive个jedis实例，则此时pool的状态为exhausted(耗尽)。
spring.redis.maxActive=1000
# 控制一个pool最多有多少个状态为idle(空闲的)的jedis实例，默认值也是8。
spring.redis.maxIdle=20
#等待可用连接的最大时间，单位毫秒，默认值为-1，表示永不超时。如果超过等待时间，则直接抛出JedisConnectionException；
spring.redis.maxWait=10000
#在borrow一个jedis实例时，是否提前进行validate操作；如果为true，则得到的jedis实例均是可用的；
spring.redis.testOnBorrow=true
spring.redis.testOnReturn=true
spring.redis.timeOut=3000
#redis超时时间 1*1*60*60
spring.redis.redisExpire=3600


#redis集群配置
spring.redis.pool.nodes=
spring.redis.pool.timeOut=3000
#返回值的超时时间
spring.redis.pool.soTimeOut=5000
spring.redis.pool.maxActive=1024
spring.redis.pool.maxIdle=200
spring.redis.pool.maxWait=10000
spring.redis.pool.maxAttempts=5
spring.redis.pool.testOnBorrow=true
spring.redis.pool.password=yss
spring.redis.pool.expireSeconds=3600

#activemq配置
activemq.username=admin
activemq.password=admin
activemq.url=tcp://172.16.1.28:61616
#文件保存路径/usr/share/nginx/html/fileupload
file.save.path=/User/lx/img

file.download.path=/User/lx/img
#ES配置
spring.data.elasticsearch.clustername=yss
spring.data.elasticsearch.ip=192.168.2.239
spring.data.elasticsearch.port=9300

#ES配置加检验
#spring.data.elasticsearch.clustername=test_cluster
#spring.data.elasticsearch.ip=10.12.20.211,10.12.20.212,10.12.20.213
#spring.data.elasticsearch.port=9300
#spring.data.elasticsearch.username=elastic
#spring.data.elasticsearch.password=yss
#spring.data.elasticsearch.xpack.keypath=f:/elastic-certificates.p12
#服务心跳上报地址
monitor.up.url=http://192.168.2.59:8080/maintenance/heart/send

spring.hadoop.paths=

spring.http.multipart.enabled=true
# 设置单个文件的最大长度
spring.http.multipart.max-file-size=100MB
# 设置最大的请求文件的大小
spring.http.multipart.max-request-size=100MB
# 当上传文件达到1MB的时候进行磁盘写入
spring.http.multipart.file-size-threshold=1MB

#spring.http.multipart.location=/usr/local/springcloud/zuul
spring.http.multipart.location=D:/tomcat/apache-tomcat-7.0.63-windows-x64/apache-tomcat-7.0.63/webapps/fileupload

##kafka-consummer
#kafka-服务地址
spring.kafka.bootstrap-servers=192.168.8.30:15007
#kafka-消费者组
spring.kafka.consumer.group-id=student_backbed
#Kafka-是否开启自动提交offset
spring.kafka.consumer.enable-auto-commit=true
#kafka-自动提交间隔
spring.kafka.consumer.auto-commit-interval=200
#kafka-序列化方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#kafka-一次最多拉取数据量
spring.kafka.consumer.max-poll-records=1
# earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
# latest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
# none:topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=earliest





